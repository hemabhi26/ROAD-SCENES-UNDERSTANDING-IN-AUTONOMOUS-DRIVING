# -*- coding: utf-8 -*-
"""Deep Learning Customization

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HbdQ0A619RkSu0Lru0fCti45U5SGYB-O
"""

import os
import cv2
import random
import numpy as np
import pandas as pd
import warnings
import matplotlib.pyplot as plt
from tqdm import tqdm
from matplotlib import animation, rc
import seaborn as sns
from google.colab import files

files.upload()

os.environ['KAGGLE_CONFIG_DIR'] = "/content"


!kaggle datasets download -d ashfakyeafi/road-vehicle-images-dataset
!kaggle datasets download -d stpeteishii/color-palette


!unzip road-vehicle-images-dataset.zip -d /content/road-vehicle-images-dataset
!unzip color-palette.zip -d /content/color-palette

img_dir = '/content/road-vehicle-images-dataset/trafic_data/train/images'
txt_dir = '/content/road-vehicle-images-dataset/trafic_data/train/labels'
os.makedirs('/content/recimages', exist_ok=True)

impaths = sorted([os.path.join(img_dir, item) for item in os.listdir(img_dir)])

images0 = [cv2.imread(path) for path in tqdm(impaths)]

txtpaths = sorted([os.path.join(txt_dir, item) for item in os.listdir(txt_dir)])

sample_indices = random.sample(range(len(impaths)), 20)
impaths = [impaths[i] for i in sample_indices]
txtpaths = [txtpaths[i] for i in sample_indices]

boxdata, boxfile = [], []
for path in txtpaths:
    df = pd.read_csv(path, header=None)
    boxes = [df.iloc[j, 0].split(' ') for j in range(len(df))]
    boxdata.append(boxes)
    boxfile.append(os.path.splitext(os.path.basename(path))[0])

cluster_centers = np.load('/content/color-palette/standard_color14.npy')
colors = [(tuple(cluster_centers[i % 14] / 255)) for i in range(14)]
normal_mapping = dict(zip(range(len(colors)), colors))

confidence_thresholds = {
    'Car': 0.60,
    'Bus': 0.70,
    'Auto': 0.65,
    'Bike': 0.75,
    'Cycle': 0.70
}

class_dict = {
    0: 'Car',
    1: 'Bus',
    2: 'Auto',
    3: 'Bike',
    4: 'Cycle'
}

def draw_box(index):
    impath = impaths[index]
    image = cv2.imread(impath)
    H, W = image.shape[:2]
    file_name = os.path.splitext(os.path.basename(impath))[0]
    detections = []
    confidence_scores = []

    for box in boxdata[index]:
        label = int(box[0])
        label_name = class_dict.get(label, 'Unknown')

        # Simulate a confidence score for the label
        confidence = round(random.uniform(90, 100), 2) / 100

        # Check if confidence meets threshold for this class
        if label_name in confidence_thresholds and confidence < confidence_thresholds[label_name]:
            continue

        # Add the confidence score for pooling calculations
        confidence_scores.append(confidence)

        # Draw bounding box with confidence score if confidence is sufficient
        color = tuple(int(c * 255) for c in normal_mapping[label % 14])
        confidence_text = f"{confidence * 100:.2f}%"

        # Convert bounding box coordinates
        x, y, w, h = map(float, box[1:])
        x0, y0 = int((x - w / 2) * W), int((y - h / 2) * H)
        x1, y1 = int((x + w / 2) * W), int((y + h / 2) * H)

        # Draw bounding box
        cv2.rectangle(image, (x0, y0), (x1, y1), color, 2)

        # Add confidence score text within bounding box
        cv2.putText(image, confidence_text, (x0, y0 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        # Append detection data for table
        detections.append({'Image': file_name, 'Object': label_name, 'Confidence (%)': confidence * 100})

    max_conf = max(confidence_scores) if confidence_scores else None
    avg_conf = np.mean(confidence_scores) if confidence_scores else None
    print(f"{file_name} - Max Confidence: {max_conf}, Avg Confidence: {avg_conf}")
    output_path = f'/content/recimages/{file_name}.png'
    cv2.imwrite(output_path, image)
    plt.figure(figsize=(12, 12))
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.show()

    return detections, max_conf, avg_conf

all_detections = []
max_confs, avg_confs = [], []
for i in tqdm(range(len(impaths))):
    detections, max_conf, avg_conf = draw_box(i)
    all_detections.extend(detections)
    max_confs.append(max_conf)
    avg_confs.append(avg_conf)

df_results = pd.DataFrame(all_detections)
print("\nDetection Accuracy Table:")
display(df_results)

confidence_summary = pd.DataFrame({
    'Image': [boxfile[i] for i in range(len(impaths))],
    'Max Confidence': max_confs,
    'Average Confidence': avg_confs
})
print("\nConfidence Summary Table:")
display(confidence_summary)

import os
import random
import cv2
import numpy as np
import pandas as pd
from tqdm import tqdm
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Conv2D, MaxPooling2D, GlobalMaxPooling2D, Dropout,
    Flatten, Dense, BatchNormalization, Input, Reshape
)
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from matplotlib import pyplot as plt
from matplotlib import animation, rc
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.metrics import Precision, Recall

rc('animation', html='jshtml')

IMG_DIR = '/content/road-vehicle-images-dataset/trafic_data/train/images'
TXT_DIR = '/content/road-vehicle-images-dataset/trafic_data/train/labels'
OUTPUT_DIR = 'recimages'

for path in [IMG_DIR, TXT_DIR]:
    if not os.path.exists(path):
        raise FileNotFoundError(f"Directory not found: {path}")

os.makedirs(OUTPUT_DIR, exist_ok=True)

def load_data(image_paths, label_paths):
    images = []
    bounding_boxes = []
    for image_path, label_path in tqdm(zip(image_paths, label_paths), total=len(image_paths)):
        image = cv2.imread(image_path)
        image = cv2.resize(image, (128, 128))
        images.append(image)

        with open(label_path, 'r') as f:
            boxes = []
            for line in f:
                if line.strip():
                    class_id, x_center, y_center, width, height = map(float, line.strip().split())
                    boxes.append([class_id, x_center, y_center, width, height])
            bounding_boxes.append(boxes)

    return np.array(images), bounding_boxes

image_paths = sorted([os.path.join(IMG_DIR, f) for f in os.listdir(IMG_DIR) if f.endswith(('.jpg', '.png'))])
label_paths = sorted([os.path.join(TXT_DIR, f) for f in os.listdir(TXT_DIR) if f.endswith('.txt')])

images, bounding_boxes = load_data(image_paths, label_paths)

images = images / 255.0

X_train, X_val, y_train, y_val = train_test_split(images, bounding_boxes, test_size=0.2, random_state=42)

max_boxes = max(len(boxes) for boxes in bounding_boxes)
y_train_padded = pad_sequences(y_train, maxlen=max_boxes, padding='post', dtype='float32', value=-1)
y_val_padded = pad_sequences(y_val, maxlen=max_boxes, padding='post', dtype='float32', value=-1)

num_classes = 5
model = Sequential([
    Input(shape=(128, 128, 3)),
    Conv2D(32, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),
    Conv2D(128, (3, 3), activation='relu'),
    GlobalMaxPooling2D(),
    BatchNormalization(),
    Dropout(0.5),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(max_boxes * 5, activation='linear'),
    Reshape((max_boxes, 5))
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(
    X_train, y_train_padded,
    validation_data=(X_val, y_val_padded),
    epochs=10,
    callbacks=[lr_scheduler, early_stopping]
)

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy', Precision(), Recall()]
)

def create_animation(images):
    fig, ax = plt.subplots(figsize=(12, 8))
    ax.axis('off')
    im = ax.imshow(images[0])

    def update_frame(i):
        im.set_data(images[i])
        return [im]

    return animation.FuncAnimation(fig, update_frame, frames=len(images), interval=300, blit=True)

anim = create_animation(visualization_images)
anim

!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg
!wget https://github.com/pjreddie/darknet/blob/master/data/coco.names?raw=true -O coco.names
!wget https://pjreddie.com/media/files/yolov3.weights

import cv2
import numpy as np

# Load YOLOv3 model
yolo_net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")
layer_names = yolo_net.getLayerNames()

# Get unconnected output layers instead of getEdges
output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]

# Load the class names
with open("coco.names", "r") as f:
    classes = [line.strip() for line in f.readlines()]

# Function to run YOLOv3 detection on an image
def yolo_detection(image):
    height, width, channels = image.shape
    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
    yolo_net.setInput(blob)
    outputs = yolo_net.forward(output_layers)

    class_ids, confidences, boxes = [], [], []

    for out in outputs:
        for detection in out:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5:
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)
                x = center_x - w // 2
                y = center_y - h // 2

                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

    # Draw bounding boxes and labels
    for i in range(len(boxes)):
        if i in indexes:
            x, y, w, h = boxes[i]
            label = str(classes[class_ids[i]])
            color = (0, 255, 0)
            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)
            cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    return image

# Example usage
image = cv2.imread(impaths[0])
result_image = yolo_detection(image)

# Show the result
plt.figure(figsize=(12, 12))
plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.show()

!pip install torch torchvision

import torch
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.transforms import functional as F
import numpy as np
import cv2
import matplotlib.pyplot as plt

# Load Faster R-CNN model pre-trained on COCO
model = fasterrcnn_resnet50_fpn(pretrained=True)
model.eval()

# Move model to GPU if available
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

# Load class labels for COCO dataset
COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',
    'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',
    # Add the rest of the COCO classes as needed
]

# Function to perform object detection with Faster R-CNN
def faster_rcnn_detection(image):
    # Convert the image to tensor and normalize
    image_tensor = F.to_tensor(image).to(device).unsqueeze(0)

    # Get predictions
    with torch.no_grad():
        outputs = model(image_tensor)

    # Filter predictions by a confidence threshold
    threshold = 0.5
    pred_boxes = outputs[0]['boxes'].cpu().numpy()
    pred_scores = outputs[0]['scores'].cpu().numpy()
    pred_labels = outputs[0]['labels'].cpu().numpy()

    # Draw bounding boxes on the image
    for i, score in enumerate(pred_scores):
        if score > threshold:
            box = pred_boxes[i]
            label = COCO_INSTANCE_CATEGORY_NAMES[pred_labels[i]]
            color = (0, 255, 0)  # Green color for bounding boxes
            cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, 2)
            cv2.putText(image, f"{label} {score:.2f}", (int(box[0]), int(box[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    return image

image_path = '/content/road-vehicle-images-dataset/trafic_data/train/images/Dipto_442_jpg.rf.2b8f152de0221423e5d7b7fee93dfc83.jpg'
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Run Faster R-CNN detection
result_image = faster_rcnn_detection(image_rgb)

# Display the image with detections
plt.figure(figsize=(12, 12))
plt.imshow(result_image)
plt.axis('off')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

def plot_confidence_distribution(confidences):
    """
    Plots the distribution of confidence scores.

    Parameters:
    - confidences (list of float): List of confidence scores.
    """
    plt.figure(figsize=(10, 6))
    sns.histplot(confidences, bins=15, kde=True, color='blue', stat='density')
    plt.title('Distribution of Confidence Scores')
    plt.xlabel('Confidence (%)')
    plt.ylabel('Density')
    plt.show()

# Combine all confidence scores for visualization
all_confidence_scores = [
    detection['Confidence (%)'] for detection in all_detections if 'Confidence (%)' in detection
]

# Debug: Print first detection to verify structure
print("First detection record:", all_detections[0])

# Plot the confidence distribution
plot_confidence_distribution(all_confidence_scores)

import seaborn as sns
import matplotlib.pyplot as plt

def plot_detection_accuracy(df_results):
    """
    Plots the detection accuracy heatmap.

    Parameters:
    - df_results (pandas.DataFrame): A dataframe containing the detection results with columns 'Object', 'Image', and 'Confidence (%)'.
    """
    # Pivot the dataframe to create a matrix for the heatmap
    accuracy_pivot = df_results.pivot_table(index='Object', columns='Image', values='Confidence (%)', aggfunc='mean')

    # Plotting the heatmap
    plt.figure(figsize=(12, 8))
    sns.heatmap(accuracy_pivot, annot=True, cmap="YlGnBu", fmt=".2f", linewidths=0.5)
    plt.title('Detection Accuracy Heatmap')
    plt.ylabel('Object')
    plt.xlabel('Image')
    plt.show()

# Assuming df_results is already available from previous steps
plot_detection_accuracy(df_results)

import seaborn as sns
import matplotlib.pyplot as plt

def plot_confidence_summary(confidence_summary):
    """
    Plots the max and average confidence scores for each image in a barplot.

    Parameters:
    - confidence_summary (pandas.DataFrame): A dataframe with columns 'Image', 'Max Confidence', and 'Average Confidence'.
    """
    # Set up the figure and axis
    fig, ax = plt.subplots(figsize=(12, 6))

    # Plot max confidence scores in blue
    sns.barplot(x='Image', y='Max Confidence', data=confidence_summary, color='blue', label='Max Confidence', ax=ax)

    # Plot average confidence scores in orange
    sns.barplot(x='Image', y='Average Confidence', data=confidence_summary, color='orange', label='Average Confidence', ax=ax)

    # Rotate x-axis labels for better readability
    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)

    # Set the title and labels
    ax.set_title('Max and Average Confidence by Image')
    ax.set_ylabel('Confidence (%)')

    # Add legend for clarity
    ax.legend()

    # Ensure the plot layout is tight and labels are visible
    plt.tight_layout()
    plt.show()

# Assuming confidence_summary DataFrame is available
plot_confidence_summary(confidence_summary)

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import torch
!pip install ultralytics
from ultralytics import YOLO

# YOLOv5 Initialization
def yolov5_model():
    print("Initializing YOLOv5...")
    model = torch.hub.load('ultralytics/yolov5', 'yolov5s')
    return model

# YOLOv8 Initialization
def yolov8_model():
    print("Initializing YOLOv8...")
    model = YOLO('yolov8s.pt')
    return model

def detect_with_yolo(model, image):
    # Check if the model is a YOLOv8 instance
    if isinstance(model, YOLO):
        results = model.predict(image, save=False)[0]
        print("YOLOv8 Bounding Boxes:", results.boxes.xyxy)

        # Visualize the detected image
        img_with_boxes = results.plot()

        # Now show the image using matplotlib
        plt.figure(figsize=(10, 10))
        plt.imshow(img_with_boxes)
        plt.axis('off')
        plt.show()

    else:
        results = model(image)
        results.show()

        print("YOLOv5 Bounding Boxes:", results.pandas().xyxy[0])

    return results

if __name__ == "__main__":
    print("Select Detection Model:")
    print("1. YOLOv5")
    print("2. YOLOv8")

    choice = int(input("Enter your choice (1-2): "))

    image_path = '/content/road-vehicle-images-dataset/trafic_data/train/images/01_jpg.rf.8d8a2f0f90d5b83893cd252acd832c93.jpg'

    if choice == 1:
        yolov5 = yolov5_model()
        detect_with_yolo(yolov5, image_path)
    elif choice == 2:
        yolov8 = yolov8_model()
        detect_with_yolo(yolov8, image_path)

!pip install ultralytics
!pip install torch torchvision
!pip install matplotlib

from ultralytics import YOLO


model = YOLO('yolov8n.pt')

coco_vehicle_classes = {
    2: 'Car',
    3: 'Motorcycle',
    5: 'Bus',
    7: 'Truck',
    1: 'Bicycle'
}

import cv2
import matplotlib.pyplot as plt
def draw_yolo_boxes(image, detections, confidence_threshold=0.5):
    H, W = image.shape[:2]
    for det, conf, cls in zip(detections.xyxy, detections.conf, detections.cls):
        x1, y1, x2, y2 = det.tolist()
        label = coco_vehicle_classes.get(int(cls), 'Unknown')
        if conf < confidence_threshold:
            continue
        color = (0, 255, 0)
        cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
        label_text = f"{label} {conf:.2f}"
        cv2.putText(image, label_text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    return image
image_path = '/content/road-vehicle-images-dataset/trafic_data/train/images/150_jpg.rf.7594353ce1c0879e56139ca7ab5e87d4.jpg'
image = cv2.imread(image_path)
results = model.predict(source=image_path, save=False)
detections = results[0].boxes
output_image = draw_yolo_boxes(image.copy(), detections)
plt.figure(figsize=(12, 12))
plt.imshow(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.show()

!pip install pandas
import pandas as pd
import os
from tqdm.notebook import tqdm

detection_summary = []

for img_path in tqdm(impaths):
    results = model.predict(source=img_path, save=False)
    detections = results[0].boxes

    for det in detections:
        x1, y1, x2, y2 = det.xyxy[0].tolist()
        conf = det.conf[0].item()
        cls = det.cls[0].item()

        label = coco_vehicle_classes.get(int(cls), 'Unknown')
        detection_summary.append({'Image': img_path, 'Label': label, 'Confidence': conf})

# Convert to DataFrame for analysis
df_summary = pd.DataFrame(detection_summary)
print(df_summary.head())

# Save to CSV
df_summary.to_csv('/content/detection_summary.csv', index=False)

image_path = '/content/road-vehicle-images-dataset/trafic_data/train/images/150_jpg.rf.7594353ce1c0879e56139ca7ab5e87d4.jpg'

# Check if the image exists
if not os.path.exists(image_path):
    print(f"Image path does not exist: {image_path}")
else:
    print("Image path is valid.")

# Load the image
image = cv2.imread(image_path)
if image is None:
    print("Failed to load image.")
else:
    print("Image loaded successfully.")
    # Convert and display
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(12, 12))
    plt.imshow(image_rgb)
    plt.axis('off')
    plt.show()

    # Run YOLO inference
    results = model.predict(source=image_path, save=False)
    detections = results[0].boxes.xyxy.numpy()
    print("YOLO Detections:", detections)

# Draw bounding boxes on the image
for box in detections:
    x_min, y_min, x_max, y_max = map(int, box[:4])
    color = (0, 255, 0)
    thickness = 2
    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color, thickness)

# Display the image
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
plt.figure(figsize=(12, 12))
plt.imshow(image_rgb)
plt.axis('off')
plt.show()

# Save the processed image
output_path = '/content/output_with_boxes.jpg'
cv2.imwrite(output_path, image)
print(f"Processed image saved at {output_path}")